{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üì∞ Analyse des actualit√©s agricoles de l'Agence Ecofin\n",
    "\n",
    "Ce notebook a pour objectif de scraper des articles li√©s √† la r√©gulation des exportations agricoles dans les pays d‚ÄôAfrique de l‚ÄôOuest, extraits du site [agenceecofin.com](https://www.agenceecofin.com/actualites-agro), puis d'extraire les donn√©es pertinentes et les sauvegarder dans un fichier Excel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import time\n",
    "import pandas as pd\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger le mod√®le de langue fran√ßais de spaCy\n",
    "nlp = spacy.load(\"fr_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialisation des variables\n",
    "mois_francais = {\n",
    "    'janvier': '01', 'f√©vrier': '02', 'fevrier': '02', 'mars': '03',\n",
    "    'avril': '04', 'mai': '05', 'juin': '06',\n",
    "    'juillet': '07', 'ao√ªt': '08', 'aout': '08', 'septembre': '09',\n",
    "    'octobre': '10', 'novembre': '11', 'd√©cembre': '12', 'decembre': '12'\n",
    "}\n",
    "\n",
    "countries = [\"B√©nin\", \"Burkina Faso\", \"Cap-Vert\", \"C√¥te d'Ivoire\", \"Gambie\", \"Ghana\", \"Guin√©e\",\n",
    "             \"Guin√©e-Bissau\", \"Lib√©ria\", \"Mali\", \"Niger\", \"Nig√©ria\", \"S√©n√©gal\", \"Sierra Leone\", \"Togo\"]\n",
    "\n",
    "keywords = [\n",
    "    \"interdiction des exportations agricoles\", \"licences d'exportation\",\n",
    "    \"embargo sur les exportations agricoles\", \"limitation des exportations agricole\",\n",
    "    \"exportation produits vivriers\", \"r√©gulation\", \"suspension des exportations agricoles\",\n",
    "    \"gouvernement suspend \", \"restriction des exportations agricoles\",\n",
    "    \"interdiction des exportations\", \"blocage\", \"plafonnement\", \"mesures protectionnistes\",\n",
    "    \"interdiction exportation arachides\", \"produits agricoles export√©s\", \"c√©r√©ales\",\n",
    "    \"bl√©\", \"riz\", \"ma√Øs\", \"acajou\", \"amande karit√©\", \"fruits et l√©gumes\", \"produits vivriers\",\n",
    "    \"produits de rente\", \"p√©nurie\", \"tubercules\", \"ni√©b√©\", \"sorgho\", \"soja\"\n",
    "]\n",
    "\n",
    "produits = [\"ma√Øs\", \"riz\", \"bl√©\", \"ni√©b√©\", \"sorgho\", \"soja\", \"cajou\", \"karit√©\", \"arachide\", \"mil\", \"tubercules\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatisation des mots-cl√©s\n",
    "keywords_lemmatized = []\n",
    "for phrase in keywords:\n",
    "    doc = nlp(phrase.lower())\n",
    "    keywords_lemmatized.append(\" \".join([token.lemma_ for token in doc]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction de normalisation de texte\n",
    "def normalize(text):\n",
    "    return text.lower().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scraping (test sur 1 page pour commencer)\n",
    "base_url = 'https://www.agenceecofin.com/actualites-agro'\n",
    "data = []\n",
    "\n",
    "for page in range(0, 10, 10):\n",
    "    url = f'{base_url}?limitstart={page}'\n",
    "    html_text = requests.get(url, timeout=50).text\n",
    "    soup = BeautifulSoup(html_text, 'lxml')\n",
    "\n",
    "    articles = soup.find_all('div', class_='catItemHeader')\n",
    "    for article in articles:\n",
    "        title_tag = article.find('div', class_='catItemTitle')\n",
    "        if title_tag:\n",
    "            title = title_tag.text.strip()\n",
    "            if (any(country.lower() in title.lower() for country in countries) and\n",
    "                any(keyword.lower() in title.lower() for keyword in keywords)):\n",
    "\n",
    "                countrie = next((country for country in countries if country.lower() in title.lower()), \"Inconnu\")\n",
    "\n",
    "                more_info = article.a['href']\n",
    "                url_article = f'https://www.agenceecofin.com{more_info}'\n",
    "                html_text2 = requests.get(url_article).text\n",
    "                soup2 = BeautifulSoup(html_text2, 'lxml')\n",
    "                read = soup2.find_all('div', class_='itemIntroText')\n",
    "\n",
    "                published_date = soup2.find('span', class_='itemDateCreated').text.strip()\n",
    "                match = re.search(r'(\\d{1,2})\\s+(\\w+)\\s+(\\d{4})', published_date.lower())\n",
    "                if match:\n",
    "                    mois_nom = match.group(2)\n",
    "                    annee = match.group(3)\n",
    "                    mois_num = mois_francais.get(mois_nom)\n",
    "                    published_date = f'{mois_num}-{annee}' if mois_num else \"Date inconnue\"\n",
    "                else:\n",
    "                    published_date = \"Date inconnue\"\n",
    "\n",
    "                texte_complet = \" \".join([normalize(p.text.strip()) for p in read])\n",
    "                texte_final = re.split(r\"lire\\s+aussi[:\\s]*\", texte_complet, flags=re.IGNORECASE)[0].strip()\n",
    "                texte_final = re.sub(r\"\\bmillions?\\b|\\bmilliards?\\b|\\bmilliers?\\b|\\bmilieu?\\b\", \"\", texte_final, flags=re.IGNORECASE)\n",
    "                texte_final = re.sub(r\"\\s{2,}\", \" \", texte_final).strip()\n",
    "\n",
    "                produits_trouves = [produit for produit in produits if normalize(produit) in texte_final]\n",
    "                produits_uniques = list(set(produits_trouves))\n",
    "\n",
    "                data.append({\n",
    "                    \"pays\": countrie,\n",
    "                    \"title\": title,\n",
    "                    \"date\": published_date,\n",
    "                    \"article\": texte_final,\n",
    "                    \"produits\": \", \".join(produits_uniques)\n",
    "                })\n",
    "\n",
    "                time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversion en DataFrame et aper√ßu\n",
    "df = pd.DataFrame(data)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export en Excel\n",
    "df.to_excel(\"articles_ecofin.xlsx\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

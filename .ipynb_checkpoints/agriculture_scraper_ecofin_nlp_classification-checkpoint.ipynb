{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e9d8d8c",
   "metadata": {},
   "source": [
    "# üì∞ Analyse des actualit√©s agricoles de l'Agence Ecofin\n",
    "\n",
    "Ce notebook a pour objectif de scraper des articles li√©s √† la r√©gulation des exportations agricoles dans les pays d‚ÄôAfrique de l‚ÄôOuest, extraits du site [agenceecofin.com](https://www.agenceecofin.com/actualites-agro), puis d'extraire les donn√©es pertinentes et les sauvegarder dans un fichier Excel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429a6b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import time\n",
    "import pandas as pd\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9257a03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger le mod√®le de langue fran√ßais de spaCy\n",
    "nlp = spacy.load(\"fr_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55620090",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialisation des variables\n",
    "mois_francais = {\n",
    "    'janvier': '01', 'f√©vrier': '02', 'fevrier': '02', 'mars': '03',\n",
    "    'avril': '04', 'mai': '05', 'juin': '06',\n",
    "    'juillet': '07', 'ao√ªt': '08', 'aout': '08', 'septembre': '09',\n",
    "    'octobre': '10', 'novembre': '11', 'd√©cembre': '12', 'decembre': '12'\n",
    "}\n",
    "\n",
    "countries = [\"B√©nin\", \"Burkina Faso\", \"Cap-Vert\", \"C√¥te d'Ivoire\", \"Gambie\", \"Ghana\", \"Guin√©e\",\n",
    "             \"Guin√©e-Bissau\", \"Lib√©ria\", \"Mali\", \"Niger\", \"Nig√©ria\", \"S√©n√©gal\", \"Sierra Leone\", \"Togo\"]\n",
    "\n",
    "keywords = [\n",
    "    \"interdiction des exportations agricoles\", \"licences d'exportation\",\n",
    "    \"embargo sur les exportations agricoles\", \"limitation des exportations agricole\",\n",
    "    \"exportation produits vivriers\", \"r√©gulation\", \"suspension des exportations agricoles\",\n",
    "    \"gouvernement suspend \", \"restriction des exportations agricoles\",\n",
    "    \"interdiction des exportations\", \"blocage\", \"plafonnement\", \"mesures protectionnistes\",\n",
    "    \"interdiction exportation arachides\", \"produits agricoles export√©s\", \"c√©r√©ales\",\n",
    "    \"bl√©\", \"riz\", \"ma√Øs\", \"acajou\", \"amande karit√©\", \"fruits et l√©gumes\", \"produits vivriers\",\n",
    "    \"produits de rente\", \"p√©nurie\", \"tubercules\", \"ni√©b√©\", \"sorgho\", \"soja\"\n",
    "]\n",
    "\n",
    "produits = [\"ma√Øs\", \"riz\", \"bl√©\", \"ni√©b√©\", \"sorgho\", \"soja\", \"cajou\", \"karit√©\", \"arachide\", \"mil\", \"tubercules\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc16d182",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatisation des mots-cl√©s\n",
    "keywords_lemmatized = []\n",
    "for phrase in keywords:\n",
    "    doc = nlp(phrase.lower())\n",
    "    keywords_lemmatized.append(\" \".join([token.lemma_ for token in doc]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1bcaf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction de normalisation de texte\n",
    "def normalize(text):\n",
    "    return text.lower().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f65831",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scraping (test sur 1 page pour commencer)\n",
    "base_url = 'https://www.agenceecofin.com/actualites-agro'\n",
    "data = []\n",
    "\n",
    "for page in range(0, 10, 10):\n",
    "    url = f'{base_url}?limitstart={page}'\n",
    "    html_text = requests.get(url, timeout=50).text\n",
    "    soup = BeautifulSoup(html_text, 'lxml')\n",
    "\n",
    "    articles = soup.find_all('div', class_='catItemHeader')\n",
    "    for article in articles:\n",
    "        title_tag = article.find('div', class_='catItemTitle')\n",
    "        if title_tag:\n",
    "            title = title_tag.text.strip()\n",
    "            if (any(country.lower() in title.lower() for country in countries) and\n",
    "                any(keyword.lower() in title.lower() for keyword in keywords)):\n",
    "\n",
    "                countrie = next((country for country in countries if country.lower() in title.lower()), \"Inconnu\")\n",
    "\n",
    "                more_info = article.a['href']\n",
    "                url_article = f'https://www.agenceecofin.com{more_info}'\n",
    "                html_text2 = requests.get(url_article).text\n",
    "                soup2 = BeautifulSoup(html_text2, 'lxml')\n",
    "                read = soup2.find_all('div', class_='itemIntroText')\n",
    "\n",
    "                published_date = soup2.find('span', class_='itemDateCreated').text.strip()\n",
    "                match = re.search(r'(\\d{1,2})\\s+(\\w+)\\s+(\\d{4})', published_date.lower())\n",
    "                if match:\n",
    "                    mois_nom = match.group(2)\n",
    "                    annee = match.group(3)\n",
    "                    mois_num = mois_francais.get(mois_nom)\n",
    "                    published_date = f'{mois_num}-{annee}' if mois_num else \"Date inconnue\"\n",
    "                else:\n",
    "                    published_date = \"Date inconnue\"\n",
    "\n",
    "                texte_complet = \" \".join([normalize(p.text.strip()) for p in read])\n",
    "                texte_final = re.split(r\"lire\\s+aussi[:\\s]*\", texte_complet, flags=re.IGNORECASE)[0].strip()\n",
    "                texte_final = re.sub(r\"\\bmillions?\\b|\\bmilliards?\\b|\\bmilliers?\\b|\\bmilieu?\\b\", \"\", texte_final, flags=re.IGNORECASE)\n",
    "                texte_final = re.sub(r\"\\s{2,}\", \" \", texte_final).strip()\n",
    "\n",
    "                produits_trouves = [produit for produit in produits if normalize(produit) in texte_final]\n",
    "                produits_uniques = list(set(produits_trouves))\n",
    "\n",
    "                data.append({\n",
    "                    \"pays\": countrie,\n",
    "                    \"title\": title,\n",
    "                    \"date\": published_date,\n",
    "                    \"article\": texte_final,\n",
    "                    \"produits\": \", \".join(produits_uniques)\n",
    "                })\n",
    "\n",
    "                time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9c4bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversion en DataFrame et aper√ßu\n",
    "df = pd.DataFrame(data)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33989489",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export en Excel\n",
    "df.to_excel(\"articles_ecofin.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9b70e4",
   "metadata": {},
   "source": [
    "## üìä Visualisation des donn√©es collect√©es\n",
    "\n",
    "Voyons maintenant quelques statistiques de base sur les articles collect√©s :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4622e0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.ticker as mtick\n",
    "\n",
    "# Configuration de style\n",
    "sns.set(style=\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb4924c",
   "metadata": {},
   "source": [
    "### Nombre d'articles par pays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e855d3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "article_counts = df['pays'].value_counts().sort_values(ascending=False)\n",
    "sns.barplot(x=article_counts.values, y=article_counts.index, palette=\"viridis\")\n",
    "plt.title(\"Nombre d'articles par pays\")\n",
    "plt.xlabel(\"Nombre d'articles\")\n",
    "plt.ylabel(\"Pays\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81bbad20",
   "metadata": {},
   "source": [
    "### R√©partition des produits agricoles mentionn√©s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db049a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "produit_list = \",\".join(df['produits'].dropna()).split(\", \")\n",
    "produit_counts = Counter(produit_list)\n",
    "produit_df = pd.DataFrame.from_dict(produit_counts, orient='index', columns=['count'])\n",
    "produit_df = produit_df.sort_values(by='count', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "sns.barplot(x=produit_df['count'], y=produit_df.index, palette=\"mako\")\n",
    "plt.title(\"Fr√©quence des produits agricoles mentionn√©s\")\n",
    "plt.xlabel(\"Occurrences\")\n",
    "plt.ylabel(\"Produits agricoles\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc297cf",
   "metadata": {},
   "source": [
    "### Nombre d'articles par ann√©e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51315f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraire l'ann√©e de la colonne 'date' au format MM-YYYY\n",
    "df['ann√©e'] = df['date'].apply(lambda x: x.split('-')[1] if '-' in x else None)\n",
    "year_counts = df['ann√©e'].value_counts().sort_index()\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.barplot(x=year_counts.index, y=year_counts.values, palette=\"crest\")\n",
    "plt.title(\"Nombre d'articles par ann√©e\")\n",
    "plt.xlabel(\"Ann√©e\")\n",
    "plt.ylabel(\"Nombre d'articles\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad1096a",
   "metadata": {},
   "source": [
    "## ü§ñ Classification NLP des articles par type d‚Äôaction\n",
    "\n",
    "Nous allons entra√Æner un mod√®le de machine learning pour pr√©dire le type d‚Äôaction (ex : interdiction, embargo, r√©gulation) √† partir du contenu des articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1cf813",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751da283",
   "metadata": {},
   "source": [
    "### Cr√©ation des labels √† partir des mots-cl√©s dans l'article\n",
    "\n",
    "On cherche dans l'article les termes typiques comme `interdiction`, `suspension`, `embargo`, etc., et on les utilise comme √©tiquette principale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6812bced",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction simple d'assignation de label\n",
    "def assign_action_label(text):\n",
    "    text = text.lower()\n",
    "    if \"interdiction\" in text:\n",
    "        return \"interdiction\"\n",
    "    elif \"embargo\" in text:\n",
    "        return \"embargo\"\n",
    "    elif \"suspension\" in text:\n",
    "        return \"suspension\"\n",
    "    elif \"r√©gulation\" in text or \"regulation\" in text:\n",
    "        return \"regulation\"\n",
    "    elif \"restriction\" in text:\n",
    "        return \"restriction\"\n",
    "    else:\n",
    "        return \"autre\"\n",
    "\n",
    "# Appliquer aux articles\n",
    "df['action_label'] = df['article'].apply(assign_action_label)\n",
    "df['action_label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec4d552",
   "metadata": {},
   "source": [
    "### Vectorisation TF-IDF et entra√Ænement du mod√®le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fb7e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrer les classes les plus fr√©quentes\n",
    "df_filtered = df[df['action_label'] != 'autre']\n",
    "\n",
    "X = df_filtered['article']\n",
    "y = df_filtered['action_label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "vectorizer = TfidfVectorizer(max_features=3000, ngram_range=(1,2), stop_words='french')\n",
    "X_train_vec = vectorizer.fit_transform(X_train)\n",
    "X_test_vec = vectorizer.transform(X_test)\n",
    "\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train_vec, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c35a0d",
   "metadata": {},
   "source": [
    "### √âvaluation du mod√®le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50df0906",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test_vec)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred, labels=model.classes_)\n",
    "sns.heatmap(cm, annot=True, fmt='d', xticklabels=model.classes_, yticklabels=model.classes_, cmap=\"Blues\")\n",
    "plt.xlabel(\"Pr√©diction\")\n",
    "plt.ylabel(\"V√©rit√© r√©elle\")\n",
    "plt.title(\"Matrice de confusion du mod√®le\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
